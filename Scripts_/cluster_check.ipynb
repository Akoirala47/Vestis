{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Fix Notebook\n",
    "## Step 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/aayus/Desktop/workstuff/Projects/Vestis/ProcessedImages\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"ProcessedImages\")\n",
    "MAX_CLUSTERS = 100  # Original number of clusters\n",
    "N_SUBCLUSTERS = 20  # Start with 20, adjust based on validation\n",
    "\n",
    "print(f\"Working directory: {PROCESSED_DIR}\")\n",
    "assert os.path.exists(PROCESSED_DIR), \"Processed directory not found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data & Identify Problem Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original cluster sizes:\n",
      "[302 344 388 451 509]\n",
      "Target cluster 21 has 509 items\n"
     ]
    }
   ],
   "source": [
    "# Load features and labels\n",
    "features = np.load(os.path.join(PROCESSED_DIR, \"all_features_reduced.npy\"))\n",
    "labels = np.load(os.path.join(PROCESSED_DIR, \"cluster_labels.npy\"))\n",
    "\n",
    "# Find largest cluster\n",
    "cluster_counts = np.bincount(labels.flatten())\n",
    "problem_cluster_id = np.argmax(cluster_counts)\n",
    "problem_mask = labels.flatten() == problem_cluster_id\n",
    "problem_features = features[problem_mask]\n",
    "\n",
    "print(f\"Original cluster sizes:\\n{np.sort(cluster_counts)[-5:]}\")\n",
    "print(f\"Target cluster {problem_cluster_id} has {problem_features.shape[0]} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sub-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 20 sub-clusters\n"
     ]
    }
   ],
   "source": [
    "def recluster_features(features, n_clusters):\n",
    "    \"\"\"Improved clustering with sanity checks\"\"\"\n",
    "    if len(features) < n_clusters:\n",
    "        n_clusters = max(1, len(features) // 5)\n",
    "        print(f\"Warning: Reducing subclusters to {n_clusters} due to small sample size\")\n",
    "    \n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        init=\"k-means++\",\n",
    "        batch_size=256,\n",
    "        random_state=42,\n",
    "        n_init=3\n",
    "    )\n",
    "    return kmeans.fit_predict(features)\n",
    "\n",
    "sub_labels = recluster_features(problem_features, N_SUBCLUSTERS)\n",
    "print(f\"Created {len(np.unique(sub_labels))} sub-clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualization & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(features, labels, n_samples=5):\n",
    "    \"\"\"Displays random samples from each sub-cluster with validation\"\"\"\n",
    "    \n",
    "    # Track changes in cluster versions\n",
    "    current_version = 1\n",
    "    \n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        print(f\"\\n--- Sub-cluster {label} ({current_version}) ---\")\n",
    "        \n",
    "        # Collect all indices for this label\n",
    "        indices = np.where(labels == label)[0]\n",
    "        \n",
    "        if len(indices) < n_samples:\n",
    "            print(f\"Only {len(indices)} items, skipping display (requires at least {n_samples})\")\n",
    "            continue\n",
    "        \n",
    "        # Get sample indices without replacement\n",
    "        samples = np.random.choice(indices, n_samples, replace=False)\n",
    "        \n",
    "        # Validate cluster quality: check for diversity in style\n",
    "        style_scores = []\n",
    "        for i, idx in enumerate(samples):\n",
    "            # Example style scoring (replace with actual logic)\n",
    "            item_features = features[idx]\n",
    "            style_score = calculate_style_similarity(item_features, [features[j] for j in samples[:i] + samples[i+1:]])\n",
    "            style_scores.append(style_score)\n",
    "        \n",
    "        # Check if all items in the cluster have similar styles\n",
    "        style_threshold = 0.5  # Adjust this threshold as needed\n",
    "        outlier_count = sum(1 for score in style_scores if score < style_threshold)\n",
    "        \n",
    "        print(f\"Cluster quality check:\")\n",
    "        print(f\"Outlier count: {outlier_count} / {n_samples}\")\n",
    "        print(f\"Style similarity scores: {style_scores}\")\n",
    "        print()\n",
    "        \n",
    "        # Replace with your actual image display code\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        for i, idx in enumerate(samples):\n",
    "            plt.subplot(1, n_samples, i+1)\n",
    "            img = load_image_from_features(features[idx])\n",
    "            plt.imshow(img)\n",
    "            if style_score < style_threshold:\n",
    "                plt.title(f\"Outlier: Style similarity {style_score:.2f}\")\n",
    "            else:\n",
    "                plt.title(f\"Normal: Style similarity {style_score:.2f}\")\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Increment version to save previous state (if needed)\n",
    "        current_version += 1\n",
    "        \n",
    "    print(\"\\nStyle Similarity Validation Checklist:\")\n",
    "    print(\"1. Do items in the same sub-cluster share similar styles?\")\n",
    "    print(f\"2. Outliers detected: {current_version - 1}\")\n",
    "    print(\"3. Are there sub-clusters that should be merged based on style similarity?\")\n",
    "    \n",
    "    # Manual validation (you can automate this with a separate function)\n",
    "    manual_validation = {\n",
    "        \"cluster_quality\": True,\n",
    "        \"outliers_in_clusters\": [label for label, count in zip(unique_labels, [style_scores.count(score < 0.5) for score_list in style_scores])],\n",
    "        \"recommend_action\": \"\"\n",
    "    }\n",
    "    \n",
    "    return manual_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved new labels to /Users/aayus/Desktop/workstuff/Projects/Vestis/ProcessedImages/cluster_labels_v1.npy\n",
      "\n",
      "Post-fix cluster sizes:\n",
      "Largest original cluster: 509\n",
      "Largest new sub-cluster: 49\n"
     ]
    }
   ],
   "source": [
    "# Create new labels array\n",
    "new_labels = labels.copy()\n",
    "new_labels[problem_mask] = MAX_CLUSTERS + sub_labels\n",
    "\n",
    "# Save with versioning\n",
    "version = 1\n",
    "while True:\n",
    "    save_path = os.path.join(PROCESSED_DIR, f\"cluster_labels_v{version}.npy\")\n",
    "    if not os.path.exists(save_path):\n",
    "        np.save(save_path, new_labels)\n",
    "        print(f\"Saved new labels to {save_path}\")\n",
    "        break\n",
    "    version += 1\n",
    "\n",
    "# Verification\n",
    "new_cluster_counts = np.bincount(new_labels.flatten())\n",
    "print(\"\\nPost-fix cluster sizes:\")\n",
    "print(f\"Largest original cluster: {np.max(cluster_counts)}\")\n",
    "print(f\"Largest new sub-cluster: {np.max(new_cluster_counts[MAX_CLUSTERS:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Update your recommendation code to use `cluster_labels_vX.npy`\n",
    "2. Run through sample recommendations to verify improvements\n",
    "3. If needed, repeat with different N_SUBCLUSTERS values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
